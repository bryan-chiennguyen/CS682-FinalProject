{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l8aHv14d_hOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16a5358c-98f2-456d-d7c1-ed1799c3f272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/682_final_proj\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_folder = '/content/drive/My Drive/682_final_proj'\n",
        "%cd {drive_folder}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pylab as plt\n",
        "import nibabel as nib\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.model_selection import KFold\n",
        "from torch import optim\n",
        "from torchvision.transforms import CenterCrop"
      ],
      "metadata": {
        "id": "I05_eBsy_qul"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor"
      ],
      "metadata": {
        "id": "0MWGKGinUrfV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resize_tensor(img_tensor, crop_size, target_size, is_seg=False):\n",
        "  depth, height, width = img_tensor.shape\n",
        "  crop_depth, crop_height, crop_width = crop_size\n",
        "  depth_start = (depth - crop_depth) // 2\n",
        "  height_start = (height - crop_height) // 2\n",
        "  width_start = (width - crop_width) // 2\n",
        "\n",
        "  cropped_img = img_tensor[depth_start:(depth_start + crop_depth)]\n",
        "\n",
        "  center_crop = CenterCrop((crop_height, crop_width))\n",
        "  cropped_img = center_crop(cropped_img)\n",
        "\n",
        "  cropped_img = cropped_img.unsqueeze(0).unsqueeze(0)\n",
        "\n",
        "  mode = 'nearest' if is_seg else 'trilinear'\n",
        "  resized_tensor = F.interpolate(cropped_img, size=target_size, mode=mode, align_corners=None if is_seg else True)\n",
        "\n",
        "  return resized_tensor"
      ],
      "metadata": {
        "id": "MIavLJZ5yMq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def weights_init_kaiming(m):\n",
        "  classname = m.__class__.__name__\n",
        "  if classname.find('Conv') != -1:\n",
        "    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "  elif classname.find('Linear') != -1:\n",
        "    nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "  elif classname.find('BatchNorm') != -1:\n",
        "    nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "    nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "def init_weights(net, init_type='kaiming'):\n",
        "  net.apply(weights_init_kaiming)"
      ],
      "metadata": {
        "id": "XbwfwjrQLbCX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UnetConv3(nn.Module):\n",
        "  def __init__(self, in_size, out_size, is_batchnorm, kernel_size=(3,3,1), padding_size=(1,1,0), init_stride=(1,1,1)):\n",
        "    super(UnetConv3, self).__init__()\n",
        "\n",
        "    if is_batchnorm:\n",
        "      self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
        "                                  nn.InstanceNorm3d(out_size),\n",
        "                                  nn.ReLU(inplace=True),)\n",
        "      self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                  nn.InstanceNorm3d(out_size),\n",
        "                                  nn.ReLU(inplace=True),)\n",
        "    else:\n",
        "      self.conv1 = nn.Sequential(nn.Conv3d(in_size, out_size, kernel_size, init_stride, padding_size),\n",
        "                                  nn.ReLU(inplace=True),)\n",
        "      self.conv2 = nn.Sequential(nn.Conv3d(out_size, out_size, kernel_size, 1, padding_size),\n",
        "                                  nn.ReLU(inplace=True),)\n",
        "\n",
        "    for m in self.children():\n",
        "      init_weights(m, init_type='kaiming')\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    outputs = self.conv1(inputs)\n",
        "    outputs = self.conv2(outputs)\n",
        "    return outputs\n",
        "\n",
        "class UnetUp3_CT(nn.Module):\n",
        "  def __init__(self, in_size, out_size, is_batchnorm=True):\n",
        "    super(UnetUp3_CT, self).__init__()\n",
        "    self.conv = UnetConv3(in_size + out_size, out_size, is_batchnorm, kernel_size=(3,3,3), padding_size=(1,1,1))\n",
        "    self.up = nn.Upsample(scale_factor=(2, 2, 2), mode='trilinear')\n",
        "\n",
        "    for m in self.children():\n",
        "      if m.__class__.__name__.find('UnetConv3') != -1: continue\n",
        "      init_weights(m, init_type='kaiming')\n",
        "\n",
        "  def forward(self, inputs1, inputs2):\n",
        "    outputs2 = self.up(inputs2)\n",
        "    offset = outputs2.size()[2] - inputs1.size()[2]\n",
        "    padding = 2 * [offset // 2, offset // 2, 0]\n",
        "    outputs1 = F.pad(inputs1, padding)\n",
        "    return self.conv(torch.cat([outputs1, outputs2], 1))"
      ],
      "metadata": {
        "id": "sTe1-jw9NXQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class unet_3D(nn.Module):\n",
        "  def __init__(self, feature_scale=8, n_classes=3, is_deconv=True, in_channels=2, is_batchnorm=True):\n",
        "    super(unet_3D, self).__init__()\n",
        "    self.is_deconv = is_deconv\n",
        "    self.in_channels = in_channels\n",
        "    self.is_batchnorm = is_batchnorm\n",
        "    self.feature_scale = feature_scale\n",
        "\n",
        "    filters = [16, 32, 64]\n",
        "    filters = [int(x / self.feature_scale) for x in filters]\n",
        "\n",
        "    self.conv1 = UnetConv3(self.in_channels, filters[0], self.is_batchnorm)\n",
        "    self.maxpool1 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "    self.conv2 = UnetConv3(filters[0], filters[1], self.is_batchnorm)\n",
        "    self.maxpool2 = nn.MaxPool3d(kernel_size=(2, 2, 2))\n",
        "\n",
        "    self.center = UnetConv3(filters[1], filters[2], self.is_batchnorm)\n",
        "\n",
        "    self.up_concat2 = UnetUp3_CT(filters[2], filters[1], is_batchnorm)\n",
        "    self.up_concat1 = UnetUp3_CT(filters[1], filters[0], is_batchnorm)\n",
        "\n",
        "    self.final = nn.Conv3d(filters[0], n_classes, 1)\n",
        "\n",
        "    self.dropout1 = nn.Dropout(p=0.2)\n",
        "    self.dropout2 = nn.Dropout(p=0.2)\n",
        "\n",
        "    # Initialise weights\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Conv3d) or isinstance(m, nn.BatchNorm3d):\n",
        "          init_weights(m, init_type='kaiming')\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    conv1 = self.conv1(inputs)\n",
        "    maxpool1 = self.maxpool1(conv1)\n",
        "\n",
        "    conv2 = self.conv2(maxpool1)\n",
        "    maxpool2 = self.maxpool2(conv2)\n",
        "\n",
        "    center = self.center(maxpool2)\n",
        "    center = self.dropout1(center)\n",
        "\n",
        "    up2 = self.up_concat2(conv2, center)\n",
        "    up1 = self.up_concat1(conv1, up2)\n",
        "    up1 = self.dropout2(up1)\n",
        "\n",
        "    final = self.final(up1)\n",
        "\n",
        "    return final"
      ],
      "metadata": {
        "id": "9-XYYy_xB-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "HC_END = 29\n",
        "BPDWOP_END = 64\n",
        "BPDWP_END = 83\n",
        "SS_END = 103\n",
        "\n",
        "N = SS_END\n",
        "\n",
        "CROP_SIZE = (128, 160, 160)\n",
        "TARGET_SIZE = (96, 120, 120)\n",
        "\n",
        "def get_dir_str(i):\n",
        "  if i <= HC_END:\n",
        "    return \"HC\"\n",
        "  elif i <= BPDWOP_END:\n",
        "    return \"BPDwoPsy\"\n",
        "  elif i <= BPDWP_END:\n",
        "    return \"BPDwPsy\"\n",
        "  else:\n",
        "    return \"SS\"\n",
        "\n",
        "def load_data(num_images):\n",
        "  images = torch.zeros(num_images, 1, 96, 120, 120).type(dtype)\n",
        "  segs = torch.zeros(num_images, 1, 96, 120, 120).type(dtype)\n",
        "  slice_ind = 0\n",
        "\n",
        "  for i in random.sample(range(1, 104), num_images):\n",
        "    dir_str = get_dir_str(i)\n",
        "    str_i = f'{i:03}'\n",
        "    dir_path = f'./final_dataset/{dir_str}/{dir_str}_{str_i}'\n",
        "\n",
        "    img_path = f'{dir_path}/{dir_str}_{str_i}_procimg.nii.gz'\n",
        "    seg_path = f'{dir_path}/{dir_str}_{str_i}_seg.nii.gz' if i <= BPDWP_END else f'{dir_path}/{dir_str}_{str_i}.seg.nii.gz'\n",
        "\n",
        "    img_tensor = torch.from_numpy(nib.load(img_path).get_fdata())\n",
        "    seg_tensor = torch.from_numpy(nib.load(seg_path).get_fdata())\n",
        "\n",
        "    img_tensor_re = resize_tensor(img_tensor, CROP_SIZE, TARGET_SIZE, is_seg=False).type(dtype)\n",
        "    seg_tensor_re = resize_tensor(seg_tensor, CROP_SIZE, TARGET_SIZE, is_seg=True).type(dtype)\n",
        "\n",
        "    images[slice_ind] = img_tensor_re\n",
        "    segs[slice_ind] = seg_tensor_re\n",
        "    slice_ind += 1\n",
        "\n",
        "  return images, segs\n",
        "\n",
        "train_images, train_segs = load_data(20)\n",
        "test_images, test_segs = load_data(5)\n",
        "\n",
        "atlas_dir_path = './final_dataset/HC/HC_013'\n",
        "atlas_img_path, atlas_seg_path = f'{atlas_dir_path}/HC_013_procimg.nii.gz', f'{atlas_dir_path}/HC_013_seg.nii.gz'\n",
        "orig_img, orig_seg = torch.from_numpy(nib.load(atlas_img_path).get_fdata()), torch.from_numpy(nib.load(atlas_seg_path).get_fdata())\n",
        "atlas_img = resize_tensor(orig_img, CROP_SIZE, TARGET_SIZE, is_seg=False).type(dtype)\n",
        "atlas_seg = resize_tensor(orig_seg, CROP_SIZE, TARGET_SIZE, is_seg=False).type(dtype)"
      ],
      "metadata": {
        "id": "1Ft1yFbTKjfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warp(img, trans_map, is_seg=False):\n",
        "  grid = trans_map.clone().permute(0, 2, 3, 4, 1)\n",
        "\n",
        "  mode = 'nearest' if is_seg else 'bilinear'\n",
        "  warped_img = torch.empty_like(img)\n",
        "  warped_img = F.grid_sample(img, grid, mode=mode, padding_mode='border', align_corners=True)\n",
        "\n",
        "  return warped_img.reshape(img.shape)"
      ],
      "metadata": {
        "id": "9JihCWYXnp5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def charbonnier(x, epsilon=0.001, gamma=0.45):\n",
        "  return (x**2 + epsilon**2)**gamma\n",
        "\n",
        "#l_sim\n",
        "def l2_image_similarity_loss(u, u_hat):\n",
        "  squared_diff = torch.square(u - u_hat)\n",
        "  return torch.mean(squared_diff)\n",
        "\n",
        "# l_cyc\n",
        "def cycle_loss(l, l_tilde):\n",
        "  return torch.mean(torch.abs(l_tilde - l))\n",
        "\n",
        "# l_anatomy_cyc\n",
        "def anatomy_cycle_loss(true_seg, reconstructed_seg):\n",
        "  all_sum = 2 * torch.sum(torch.mul(true_seg, reconstructed_seg))\n",
        "  true_seg_sum = torch.sum(torch.square(true_seg))\n",
        "  reconstructed_seg_sum = torch.sum(torch.square(reconstructed_seg))\n",
        "  return 1 - (all_sum / (true_seg_sum + reconstructed_seg_sum))\n",
        "\n",
        "# l_diff_cyc\n",
        "def cycle_transformation_loss(forward_map, backward_map):\n",
        "  return charbonnier(torch.add(forward_map, backward_map)).sum()\n",
        "\n",
        "# l_diff_cyc\n",
        "def diff_cyc_loss(l_s, u_s_hat, l_s_hat):\n",
        "  return torch.sum(charbonnier(torch.abs(l_s - u_s_hat) - torch.abs(u_s_hat - l_s)))\n",
        "\n",
        "def total_loss(weight1, weight2, img, f_warp_img, atlas, re_atlas, true_seg, f_warp_seg, re_seg, f_map, b_map):\n",
        "  return l2_image_similarity_loss(img, f_warp_img) + weight1 * cycle_loss(atlas, re_atlas) + weight2 * (anatomy_cycle_loss(true_seg, re_seg) + cycle_transformation_loss(f_map, b_map) + diff_cyc_loss(true_seg, f_warp_seg, re_seg))"
      ],
      "metadata": {
        "id": "mwRHxx9RqopG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dice_score(x, y):\n",
        "  x_flat = x.view(-1)\n",
        "  y_flat = y.view(-1)\n",
        "\n",
        "  intersection = (x_flat == y_flat).sum()\n",
        "  union = x_flat.size(dim=0) + y_flat.size(dim=0)\n",
        "\n",
        "  dice = (2.0 * intersection) / union\n",
        "  return dice"
      ],
      "metadata": {
        "id": "DPjwZIPt-bBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer():\n",
        "  def __init__(self):\n",
        "    self.forward_model = unet_3D().type(dtype)\n",
        "    self.backward_model = unet_3D().type(dtype)\n",
        "    self.optimizer_f = optim.Adam(self.forward_model.parameters(), lr=0.0002)\n",
        "    self.optimizer_b = optim.Adam(self.backward_model.parameters(), lr=0.0002)\n",
        "\n",
        "  def fgen_forward(self, x):\n",
        "    return self.forward_model(x)\n",
        "\n",
        "  def bgen_forward(self, x):\n",
        "    return self.backward_model(x)"
      ],
      "metadata": {
        "id": "CF9vuI3PLQDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, img_batch, seg_batch, num_epochs, l_weight1, l_weight2):\n",
        "  for i in range(num_epochs):\n",
        "    model.optimizer_f.zero_grad()\n",
        "    model.optimizer_b.zero_grad()\n",
        "\n",
        "    atlas_expand = atlas_img.expand_as(img_batch)\n",
        "\n",
        "    concat_f_input = torch.cat((img_batch, atlas_expand), dim=1)\n",
        "    f_map = model.fgen_forward(concat_f_input)\n",
        "    f_warp_img = warp(atlas_expand, f_map, is_seg=False)\n",
        "    f_warp_seg = warp(seg_batch, f_map, is_seg=True)\n",
        "\n",
        "    concat_b_input = torch.cat((f_warp_img, atlas_expand), dim=1)\n",
        "    b_map = model.bgen_forward(concat_b_input)\n",
        "    b_warp_img = warp(f_warp_img, b_map, is_seg=False)\n",
        "    b_warp_seg = warp(f_warp_seg, b_map, is_seg=True)\n",
        "\n",
        "    combined_loss = total_loss(l_weight1, l_weight2, img_batch, f_warp_img, atlas_expand, b_warp_img, seg_batch, f_warp_seg, b_warp_seg, f_map, b_map)\n",
        "\n",
        "    combined_loss.backward()\n",
        "\n",
        "    # Update model parameters\n",
        "    model.optimizer_f.step()\n",
        "    model.optimizer_b.step()\n",
        "\n",
        "    print(f\"Iteration {i + 1}: Combined Loss = {combined_loss}\")"
      ],
      "metadata": {
        "id": "w2zgA2aLOlSD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation(model, test_images, test_segs):\n",
        "  atlas_img_expand = atlas_img.expand_as(test_images)\n",
        "  atlas_seg_expand = atlas_seg.expand_as(test_segs)\n",
        "\n",
        "  concat_f_input = torch.cat((test_images, atlas_img_expand), dim=1)\n",
        "  f_map = model.fgen_forward(concat_f_input)\n",
        "  # f_warp_img = warp(atlas_img_expand, f_map, is_seg=False)\n",
        "  f_warp_seg = warp(atlas_seg_expand, f_map, is_seg=True)\n",
        "\n",
        "  seg_dice = dice_score(f_warp_seg, test_segs)\n",
        "\n",
        "  return seg_dice"
      ],
      "metadata": {
        "id": "rqRMArfOMjbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def k_fold_training(train_images, train_segs, k=5, num_epochs=15, lr=0.0002, l_weight1=10, l_weight2=3):\n",
        "  kf = KFold(n_splits=k, shuffle=True)\n",
        "\n",
        "  for fold, (train_index, val_index) in enumerate(kf.split(train_images)):\n",
        "    X_train, X_val = train_images[train_index], train_images[val_index]\n",
        "    y_train, y_val = train_segs[train_index], train_segs[val_index]\n",
        "\n",
        "    model = Trainer(lr)\n",
        "\n",
        "    train(model, X_train, y_train, num_epochs)\n",
        "    val_dice_score = validation(model, X_val, y_val)\n",
        "\n",
        "    print(f\"Fold {fold + 1} - DICE SCORE: {val_dice_score}\")\n",
        "\n",
        "# k_fold_training(train_images, train_segs, 5, 15, 0.0002, 5, 2)"
      ],
      "metadata": {
        "id": "lXSvFlwqMHhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Trainer()\n",
        "\n",
        "train(model, train_images, train_segs, num_epochs=15, l_weight1=5, l_weight2=2)\n",
        "test_dice_score = validation(model, test_images, test_segs)\n",
        "print(\"TEST SET DICE SCORE:\", test_dice_score)"
      ],
      "metadata": {
        "id": "qt4VHXRTkRwY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54d8f9fc-6d1f-4bc5-eb8f-62c4ece1aa22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: Combined Loss = 237143824.0\n",
            "Iteration 2: Combined Loss = 235611376.0\n",
            "Iteration 3: Combined Loss = 234480640.0\n",
            "Iteration 4: Combined Loss = 233460592.0\n",
            "Iteration 5: Combined Loss = 232449040.0\n",
            "Iteration 6: Combined Loss = 231337472.0\n",
            "Iteration 7: Combined Loss = 230258368.0\n",
            "Iteration 8: Combined Loss = 229210384.0\n",
            "Iteration 9: Combined Loss = 228159760.0\n",
            "Iteration 10: Combined Loss = 227096640.0\n",
            "Iteration 11: Combined Loss = 226300288.0\n",
            "Iteration 12: Combined Loss = 225391648.0\n",
            "Iteration 13: Combined Loss = 224591984.0\n",
            "Iteration 14: Combined Loss = 223901104.0\n",
            "Iteration 15: Combined Loss = 223207328.0\n",
            "TEST SET DICE SCORE: tensor(0.6878, device='cuda:0')\n"
          ]
        }
      ]
    }
  ]
}